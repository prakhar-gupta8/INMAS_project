{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b9dcfec-8b5a-4b14-9f26-be3c0f15773d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags and backslashes removal: success.\n",
      "Shortcuts expansion: success.\n",
      "Extra symbol removal: success.\n",
      "Capitalized letter removal: success.\n",
      "Initiating merged word separation...\n",
      "20 out of 1000 sentences processed.\n",
      "40 out of 1000 sentences processed.\n",
      "60 out of 1000 sentences processed.\n",
      "80 out of 1000 sentences processed.\n",
      "100 out of 1000 sentences processed.\n",
      "120 out of 1000 sentences processed.\n",
      "140 out of 1000 sentences processed.\n",
      "160 out of 1000 sentences processed.\n",
      "180 out of 1000 sentences processed.\n",
      "200 out of 1000 sentences processed.\n",
      "220 out of 1000 sentences processed.\n",
      "240 out of 1000 sentences processed.\n",
      "260 out of 1000 sentences processed.\n",
      "280 out of 1000 sentences processed.\n",
      "300 out of 1000 sentences processed.\n",
      "320 out of 1000 sentences processed.\n",
      "340 out of 1000 sentences processed.\n",
      "360 out of 1000 sentences processed.\n",
      "380 out of 1000 sentences processed.\n",
      "400 out of 1000 sentences processed.\n",
      "420 out of 1000 sentences processed.\n",
      "440 out of 1000 sentences processed.\n",
      "460 out of 1000 sentences processed.\n",
      "480 out of 1000 sentences processed.\n",
      "500 out of 1000 sentences processed.\n",
      "520 out of 1000 sentences processed.\n",
      "540 out of 1000 sentences processed.\n",
      "560 out of 1000 sentences processed.\n",
      "580 out of 1000 sentences processed.\n",
      "600 out of 1000 sentences processed.\n",
      "620 out of 1000 sentences processed.\n",
      "640 out of 1000 sentences processed.\n",
      "660 out of 1000 sentences processed.\n",
      "680 out of 1000 sentences processed.\n",
      "700 out of 1000 sentences processed.\n",
      "720 out of 1000 sentences processed.\n",
      "740 out of 1000 sentences processed.\n",
      "760 out of 1000 sentences processed.\n",
      "780 out of 1000 sentences processed.\n",
      "800 out of 1000 sentences processed.\n",
      "820 out of 1000 sentences processed.\n",
      "840 out of 1000 sentences processed.\n",
      "860 out of 1000 sentences processed.\n",
      "880 out of 1000 sentences processed.\n",
      "900 out of 1000 sentences processed.\n",
      "920 out of 1000 sentences processed.\n",
      "940 out of 1000 sentences processed.\n",
      "960 out of 1000 sentences processed.\n",
      "980 out of 1000 sentences processed.\n",
      "1000 out of 1000 sentences processed.\n",
      "Merged word separation: success.\n",
      "Blob processing: success.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from wordsegment import load, segment\n",
    "load()\n",
    "\n",
    "# Load the data.\n",
    "df = pd.read_csv('IMDB Dataset.csv') # Load the dataset.\n",
    "'''\n",
    "N_sample = 1000 # Set the testing sample size.\n",
    "df = df.sample(N_sample) # Select a small subset for testing.\n",
    "'''\n",
    "df['sentiment'] = (df['sentiment'] == 'positive').astype(int) # Convert the 'sentiment' values to 0/1.\n",
    "text_list = df['review'].to_list() # Get the list of reviews as strings.\n",
    "N = len(text_list) # Size of the dataset.\n",
    "\n",
    "# Remove the tags and backslashes.\n",
    "def remove_tags_backslashes(df, text_list, verbose=True):\n",
    "    text_list = [re.sub(r'<.*?>', '', text) for text in text_list] # Remove the tags.\n",
    "    text_list = [re.sub(r'\\\\.', '', text) for text in text_list] # Remove the backslashes with symbols.\n",
    "    if verbose:\n",
    "        print ('Tags and backslashes removal: success.')\n",
    "    df['review'] = text_list\n",
    "    return (df, text_list)\n",
    "\n",
    "# Convert shortcuts like \"who'll\" to full sentences like \"who will\".\n",
    "def expand_contractions(df, text_list, verbose=True):\n",
    "    contractions_dict = {\"n't\": \" not\", \"'ll\": \" will\", \"'ve\": \" have\", \"'re\": \" are\", \"'d\": \" would\",\n",
    "        \"'s\": \" is\", \"'m\": \" am\", \"'cause\": \" because\", \"y'all\": \"you all\",\n",
    "        \"o'clock\": \"of the clock\", \"won't\": \"will not\", \"can't\": \"cannot\",\n",
    "        \"gonna\": \"going to\", \"wanna\": \"want to\", \"gotta\": \"got to\"} # Dictionary of contraction substitutions.\n",
    "    def expand(text): # Function expanding contractions.\n",
    "        for contraction, expanded in contractions_dict.items():\n",
    "            text = re.sub(re.escape(contraction), expanded, text) # Do the expansions.\n",
    "        return text\n",
    "    text_list = [expand(text) for text in text_list] # Expand all contractions.\n",
    "    if verbose:\n",
    "        print ('Shortcuts expansion: success.')\n",
    "    df['review'] = text_list\n",
    "    return (df, text_list)\n",
    "\n",
    "# Process the extraneous symbols.\n",
    "def process_extra_symbols(df, text_list, verbose=True):\n",
    "    f_extra_symbols = [len(re.findall(r'[^a-zA-Z0-9\\s]', text)) for text in text_list] # Get the list of punctuation numbers.\n",
    "    text_list = [re.sub(r'[^a-zA-Z0-9\\s]', '', text) for text in text_list] # Remove extraneous symbols.\n",
    "    text_list = [re.sub(r'\\s+', ' ', text).strip() for text in text_list] # Assure single space separation.\n",
    "    f_extra_symbols = [f/(1 + text.count(' ')) for f, text in zip(f_extra_symbols, text_list)] # Get the fractions of extraneous symbols.\n",
    "    df['f Punctuation'] = f_extra_symbols # Add them to the dataframe.\n",
    "    if verbose:\n",
    "        print ('Extra symbol removal: success.')\n",
    "    df['review'] = text_list\n",
    "    return (df, text_list)\n",
    "\n",
    "# Process the capitalized letters.\n",
    "def process_caps(df, text_list, verbose=True):\n",
    "    f_caps = [sum(1 for word in text.split() if word.isupper()) for text in text_list] # Get the list of numbers of capitalized letters.\n",
    "    text_list = [text.lower() for text in text_list] # Decapitalize all sentences.\n",
    "    f_caps = [f/len(re.sub(' ', '', text)) for f, text in zip(f_caps, text_list)] # Get the fractions of capitalized letters.\n",
    "    df['f Capitalized'] = f_caps # Add them to the dataframe.\n",
    "    if verbose:\n",
    "        print ('Capitalized letter removal: success.')\n",
    "    df['review'] = text_list\n",
    "    return (df, text_list)\n",
    "\n",
    "# Separate the merged words.\n",
    "def separate_merged_words(df, text_list, N, verbose=True, N_updates=50):\n",
    "    if verbose:\n",
    "        print (\"Initiating merged word separation...\")\n",
    "    f_errors = [0] * N # Initialize the error count.\n",
    "    T_updates = N//N_updates if N_updates > 0 else None # Calculate the period of the diagnostic output.\n",
    "    def fix_merged_words(text): # Function returning a sentence with separated merged words.\n",
    "        old_words = text.split() # Tokenize by spaces.\n",
    "        new_words = [] # Initialize the list of corrected words.\n",
    "        for word in old_words: # Loop over all original words.\n",
    "            new_words = new_words + segment(word) # Split and append them to the list.\n",
    "        return \" \".join(new_words) # Return the corrected string.\n",
    "    for i in range(N): # Loop over all reviews.\n",
    "        text_updated = fix_merged_words(text_list[i]) # Split all words in a review. \n",
    "        f_errors[i] = len(text_list[i]) - len(text_updated) # Count them as errors.\n",
    "        text_list[i] = text_updated[:] # Update the review.\n",
    "        if verbose and (not (T_updates is None)) and (i+1)%T_updates == 0:\n",
    "            print (str(i+1) + ' out of ' + str(N) + ' sentences processed.')\n",
    "    if verbose:\n",
    "        print ('Merged word separation: success.')\n",
    "    df['review'] = text_list\n",
    "    return (df, text_list, f_errors)\n",
    "\n",
    "# Correct spelling errors.\n",
    "def correct_errors(df, text_list, N, f_errors=None, verbose=True, N_updates=50):\n",
    "    if verbose:\n",
    "        print (\"Initiating the error correction process...\")\n",
    "    if f_errors is None: # If the vector of space errors was not supplied...\n",
    "        f_errors = [0] * N # ... then initialize the error count.\n",
    "    print ('1')\n",
    "    T_updates = N//N_updates if N_updates > 0 else None # Calculate the period of the diagnostic output.\n",
    "    print ('2')\n",
    "    for i in range(N): # Loop ovr all entries.\n",
    "        blob = TextBlob(text_list[i]) # Initialize the spell checker.\n",
    "        print ('3')\n",
    "        words = text_list[i].split() # Split the words.\n",
    "        print ('4')\n",
    "        words_corrected = blob.correct().words # Correct the words.\n",
    "        print ('5')\n",
    "        f_errors[i] += sum(1 for word in words if not (word in words_corrected)) # Find the number of incorrect words.\n",
    "        print ('6')\n",
    "        text_list[i] = str(blob.correct()).lower() # Update the review. \n",
    "        print ('7')\n",
    "        f_errors[i] = f_errors[i] / (5 + len(words)) # Count the regularized number of spelling errors.\n",
    "        print ('8')\n",
    "        if verbose and (not (T_updates is None)) and (i+1)%T_updates == 0:\n",
    "            print (str(i+1) + ' out of ' + str(N) + ' sentences processed.')\n",
    "    df['f Errors'] = f_errors # Add them to the dataframe.\n",
    "    if verbose:\n",
    "        print ('Error correction: success.')\n",
    "    df['review'] = text_list\n",
    "    return (df, text_list)\n",
    "\n",
    "# Perform blob sentiment analysis.\n",
    "def blob_analyze_sentiment(df, text_list, N, verbose=True):\n",
    "    sentiment_polarity_list = [0] * N # Initialize the list of polarity scores.\n",
    "    sentiment_subjectivity_list = [0] * N # Initialize the list of subjectivity scores.\n",
    "    for i in range(N): # Loope over the dataset. \n",
    "        blob = TextBlob(text_list[i]) # Initialize the sentiment analyzer.\n",
    "        sentiment_polarity_list[i] = (blob.sentiment.polarity + 1) / 2 # Obtain the polarity score.\n",
    "        sentiment_subjectivity_list[i] = 1 - blob.sentiment.subjectivity # Obtain the subjectivity score.\n",
    "    df['Blob subjectivity score'] = sentiment_subjectivity_list # Add the subjectivity scores to the dataframe.\n",
    "    df['Blob polarity score'] = sentiment_polarity_list # Add the polarity scores to the dataframe.\n",
    "    if verbose:\n",
    "        print ('Blob processing: success.')\n",
    "    return df\n",
    "        \n",
    "# RUN THE SELECTED PRE-PROCESSING PROCEDURES. UNCOMMENT THE ONES TO BE RUN.\n",
    "df, text_list = remove_tags_backslashes(df, text_list)\n",
    "df, text_list = expand_contractions(df, text_list)\n",
    "df, text_list = process_extra_symbols(df, text_list)\n",
    "df, text_list = process_caps(df, text_list)\n",
    "f_errors = None\n",
    "df, text_list, f_errors = separate_merged_words(df, text_list, N)\n",
    "#df, text_list = correct_errors(df, text_list, N, f_errors)\n",
    "df = blob_analyze_sentiment(df, text_list, N)\n",
    "\n",
    "df.to_csv('IMDB_dataset_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44e930-b3f5-4482-9a65-9ceb87a08d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py312]",
   "language": "python",
   "name": "conda-env-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
